
\bf{Problem 8.3}
\\
\\ \text{Let} $ X_{1}, X_{2},...,X_{n} $ \text{be a random sample of size n from a normal distribution,} $ X_{i} $ \text{is appoximately} $ N(\mu,\sigma^{2}), $ \text{and define} $ U = \sum\limits_{i=1}^n X_{i} $ \text{and} $ W = \sum\limits_{i=1}^n X_{i}^{2} $
\\
\\ \text{a) Find a statistic that is a function of U and W and unbiased for the parameter} $ \theta = 2\mu-5\sigma^{2} $
\\ \[ \mu = E(\bar{x})=E(\dfrac{\sum\limits_{i=1}^n X_{i}}{n})=\dfrac{1}{n}E(U) \]
\\ \[ \sigma^{2} = E(S^{2}) = E (\dfrac{\sum\limits_{i=1}^n X_{i}^{2} - n\bar{x}^{2})}{n-1}) = \dfrac{1}{n-1}E(W-n(\dfrac{(\sum\limits_{i=1}^n X_{i})^{2}}{n^{2}}) = \dfrac{1}{n-1}E(W-\dfrac{U^{2}}{n}) = E(\dfrac{1}{n-1}(W-\dfrac{U^{2}}{n}))\]
\\ \[ \theta = 2\mu-5\sigma^{2} = 2(\dfrac{1}{n}E(U))-5E(\dfrac{1}{n-1}(W-\dfrac{U^{2}}{n})) = E(\dfrac{2}{n}(U) - \dfrac{5}{n-1}(W-\dfrac{U^{2}}{n})) = \dfrac{2U}{n}-\dfrac{5}{n-1}(W-\dfrac{U^{2}}{n}) \] \text {which is an unbiased estimator for} $ \theta $
\\
\\ \text{b) Find a statistic that is unbiased for} $\sigma^{2}+\mu^{2} $ 
\\ \[ \mu=E(\dfrac{\sum\limits_{i=1}^n X_{i}}{n}) \]
\\ \[ \mu^{2} = (E(\dfrac{\sum\limits_{i=1}^n X_{i}}{n}))^{2} = E((\dfrac{\sum\limits_{i=1}^n X_{i}}{n})^{2}) - Var(\dfrac{\sum\limits_{i=1}^n X_{i}}{n}) = E(\dfrac{1}{n^{2}}(\sum\limits_{i=1}^n X_{i})^{2})-\dfrac{1}{n^{2}}Var(\sum\limits_{i=1}^n X_{i}) = \]
\\ \[E(\dfrac{U^{2}}{n^{2}})-\dfrac{1}{n^{2}}n\sigma^{2} = E(\dfrac{U^{2}}{n^{2}})-\dfrac{\sigma^{2}}{n} \]
\\ \[ \sigma^{2} = \dfrac{1}{n-1}E(W-\dfrac{U^{2}}{n}) \]
\\ \[ \sigma^{2}+\mu^{2} = \sigma^{2}+E(\dfrac{U^{2}}{n^{2}})-\dfrac{\sigma^{2}}{n} = \dfrac{n-1}{n}\sigma^{2}+E(\dfrac{U^{2}}{n^{2}}) = \dfrac{n-1}{n}\dfrac{1}{n-1}E(W-\dfrac{U^{2}}{n})+E(\dfrac{U^{2}}{n^{2}}) = \] \\ \[E(\dfrac{W}{n}-\dfrac{U^{2}}{n})+E(\dfrac{U^{2}}{n^{2}}) = E(\dfrac{W}{n}) = \dfrac{W}{n} \]
\text {which is an unbiased estimator for} $ \sigma^{2}+\mu^{2} $
\\
\\ \text{c) Let c be a constant, and define} $Y_{i}=1 $ \text {if} $ X_{i}\leq c $ \text {and zero otherwise. Find a statistic that is a} 
\\ \text {function of} $Y_{1}, Y_{2}, ... Y_{n}$ \text {and also unbiased for} $F_{x}(c)=\phi(\dfrac{c-\mu}{\sigma}) $
\\ \[ P(Y_{i}=1) = P(X_{i}\leq c) = P(\dfrac{X_{i}-\mu}{\sigma} \leq \dfrac{c-\mu}{\sigma}) = P(N(0,1) \leq \dfrac{c-\mu}{\sigma}) = \phi(\dfrac{c-\mu}{\sigma}) = F_{x}(c) \]
\\ \[ E(Y_{i}) = 1(P(Y_{i}=1)) + 0(P(Y_{i}=0)) = P(Y_{i}=1) = \phi(\dfrac{c-\mu}{\sigma}) \]
\\ \[ E(\dfrac{1}{n}\sum\limits_{i=1}^n Y_{i}) = \dfrac{1}{n}\sum\limits_{i=1}^n E(Y_{i}) = \dfrac{1}{n} n\phi(\dfrac{c-\mu}{\sigma}) = \phi(\dfrac{c-\mu}{\sigma}) \]
\\ \[ \dfrac{1}{n}\sum\limits_{i=1}^n Y_{i} = \bar{Y} \]
\\ \text {which is unbiased for} $ F_{x}(c) = \phi(\dfrac{c-\mu}{\sigma})$